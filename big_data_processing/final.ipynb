{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "os.environ[\"HADOOP_HOME\"] = \"C:\\\\hadoop-3.3.5\"\n",
    "\n",
    "# Set hadoop.home.dir system property\n",
    "os.environ[\"hadoop.home.dir\"] = \"C:\\\\hadoop-3.3.5\"\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"ebay_analysis\").getOrCreate()\n",
    "\n",
    "# File location and type\n",
    "file_location = \"Data\\\\Data with coordinates\\\\Final_Dataset.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"false\"\n",
    "first_row_is_header = \"false\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# Read data from CSV file into a DataFrame\n",
    "final_dataset = spark.read.format(file_type) \\\n",
    "    .option(\"inferSchema\", infer_schema) \\\n",
    "    .option(\"header\", first_row_is_header) \\\n",
    "    .option(\"sep\", delimiter) \\\n",
    "    .load(file_location)\n",
    "\n",
    "# Show the DataFrame\n",
    "final_dataset.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e1bcbf3258980e5f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "final_dataset = final_dataset.dropna()\n",
    "final_dataset.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d1a8c406b9c75ea"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "column_names = [\"Location\",\"Latitude\", \"Longitude\", \"Gender\", \"Title\", \"Price\", \"Total Sold Items\", \"Total Available Items\",\"Rating\", \"Seller Name\", \"Seller Feedback\", \"Product Condition\", \"URL\", \"Category\"]\n",
    "\n",
    "# Assuming data is your DataFrame with the default column names\n",
    "final_dataset = final_dataset.toDF(*column_names)\n",
    "\n",
    "final_dataset_with_id = final_dataset.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "\n",
    "# Identify the row to be deleted (in this case, the first row)\n",
    "final_dataset = final_dataset_with_id.filter(\"row_id != 0\")\n",
    "\n",
    "# Drop the identifier column if not needed\n",
    "final_dataset = final_dataset.drop(\"row_id\")\n",
    "\n",
    "\n",
    "# Show the result or proceed with further analysis\n",
    "final_dataset.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e532ed986c732b1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "final_dataset = final_dataset.withColumn(\"PID\", monotonically_increasing_id())\n",
    "# women_clothing_df = women_clothing_df.select(\"PID\", *women_clothing_df.columns)\n",
    "# Show the DataFrame with the new \"PID\" column\n",
    "final_dataset = final_dataset.select('PID', 'Category', *[col for col in final_dataset.columns if col not in ['PID', 'Category']])\n",
    "\n",
    "\n",
    "final_dataset.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64372771e8a65403"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, col, regexp_extract, when, rand, round\n",
    "\n",
    "final_dataset_df = final_dataset.withColumn(\"Price\", regexp_replace(col(\"Price\"), \"[^0-9.]\", \"\"))\n",
    "\n",
    "# Convert the \"Price\" column to a numeric format\n",
    "final_dataset_df = final_dataset_df.withColumn(\"Price\", col(\"Price\").cast(\"float\"))\n",
    "\n",
    "final_dataset_df = final_dataset_df.withColumn(\"Total Sold Items\",\n",
    "                                                           regexp_extract(col(\"Total Sold Items\"), r'(\\d+)', 1).cast(\"integer\"))\n",
    "\n",
    "final_dataset_df = final_dataset_df.withColumn(\"Rating\",\n",
    "                                                           when(col(\"Rating\") == \"Not Available\",\n",
    "                                                                (round(rand() * 9 + 1) / 2).cast(\"float\"))\n",
    "                                                           .otherwise(col(\"Rating\").cast(\"float\")))\n",
    "\n",
    "final_dataset_df = final_dataset_df.withColumn(\"Total Available Items\",\n",
    "                                                           regexp_extract(col(\"Total Available Items\"), r'(\\d+)', 1).cast(\"integer\"))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "641e241043101308"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "final_dataset_df.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f7ff4c0327f56f9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "final_dataset_df = final_dataset_df.filter(final_dataset_df['Location'] != 'Not Available')\n",
    "\n",
    "final_dataset_df.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b691eae18c88ab24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, rand\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Assuming 'final_dataset_df' is your DataFrame\n",
    "# Generate random values between 1 and 50\n",
    "min_value = 1\n",
    "max_value = 50\n",
    "\n",
    "# Replace null values with random values\n",
    "final_dataset_df = final_dataset_df.withColumn(\n",
    "    'Total Sold Items',\n",
    "    when(col('Total Sold Items').isNull(), (rand() * (max_value - min_value) + min_value).cast('int'))\n",
    "    .otherwise(col('Total Sold Items'))\n",
    ")\n",
    "\n",
    "final_dataset_df = final_dataset_df.withColumn(\n",
    "    'Total Available Items',\n",
    "    when(col('Total Available Items').isNull(), (rand() * (max_value - min_value) + min_value).cast('int'))\n",
    "    .otherwise(col('Total Available Items'))\n",
    ")\n",
    "\n",
    "final_dataset_df.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fdb295e409f33203"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "final_dataset_df.coalesce(1).write.csv(\"E:\\\\Ebay Data Analysis\\\\Ebay_Analysis\\\\Output\", header=True, mode=\"overwrite\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cefbb5f26bedfb86"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "final_dataset_df.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d99130e53201e9c4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Top categories according to location - latitude - longitude\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank\n",
    "\n",
    "\n",
    "# Define a window specification for ranking categories within each location\n",
    "window_spec = Window.partitionBy('Location').orderBy(desc('Total Sold Items'))\n",
    "\n",
    "# Rank the categories based on total sold items within each location\n",
    "ranked_df = final_dataset_df.withColumn('rank', rank().over(window_spec))\n",
    "\n",
    "# Select the top-selling categories for each location\n",
    "top_categories_df = ranked_df.filter(col('rank') == 1).select('Category','Seller Name','Title','Price', 'Location', 'Latitude', 'Longitude', 'Total Sold Items')\n",
    "\n",
    "\n",
    "top_categories_df = top_categories_df.filter(~col('Category').contains('http'))\n",
    "top_categories_df = top_categories_df.limit(50)\n",
    "\n",
    "top_categories_df.coalesce(1).write.csv(\"E:\\\\Ebay Data Analysis\\\\Ebay_Analysis\\\\Output\", header=True, mode=\"overwrite\")\n",
    "top_categories_df.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc398afdace3988b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Top sellers category wise\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "# Group by Category and Seller Name, then calculate the total number of items sold\n",
    "top_sellers_category_wise_df = (\n",
    "    final_dataset_df\n",
    "    .groupBy('Category', 'Seller Name')\n",
    "    .agg({'Total Sold Items': 'sum'})\n",
    "    .withColumnRenamed('sum(Total Sold Items)', 'Total Items Sold')\n",
    "    .orderBy('Category', desc('Total Items Sold'))\n",
    ")\n",
    "\n",
    "# Window function to assign row numbers based on total items sold within each category\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "windowSpec = Window.partitionBy('Category').orderBy(desc('Total Items Sold'))\n",
    "\n",
    "# Add a row number column to get the top seller within each category\n",
    "top_sellers_category_wise_df = (\n",
    "    top_sellers_category_wise_df\n",
    "    .withColumn('row_num', row_number().over(windowSpec))\n",
    "    .filter(col('row_num') == 1)\n",
    "    .drop('row_num')\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "top_sellers_category_wise_df.show(truncate=False)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3219a390c521346d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result_df.show(20, truncate=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8b37a08f277d156"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "final_dataset_df.select('Category').distinct().show(truncate=False)\n",
    "final_dataset_df.filter(col('Category').isNotNull() & (col('Category') != \"\")).show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6584cb3f895bf792"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a41ce214e4b1464b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
